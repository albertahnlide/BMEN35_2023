{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antfolk/BMEN35_2023/blob/main/Session5/BMEN35_Ex16_deep_neural_networks_assignment5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezggec-uWg-6"
      },
      "source": [
        "# Assignment 5\n",
        "## Fill in your name below\n",
        "--- YOUR NAME GOES HERE ---\n",
        "\n",
        "## Your mission is now the following:\n",
        "\n",
        "You will use a (a modified version of a) dataset named HAM10000 (\"Human Against Machine with 10000 training images\")  (https://doi.org/10.7910/DVN/DBW86T). The dataset contains dermatoscopic images from different populations, acquired and stored by different modalities. The original dataset consists of 10015 dermatoscopic images, in 600x450x3 pixel resolution, which can serve as a training set for academic machine learning purposes.  Cases include a representative collection of all important diagnostic categories in the realm of pigmented lesions: Actinic keratoses and intraepithelial carcinoma / Bowen's disease (akiec), basal cell carcinoma (bcc), benign keratosis-like lesions (solar lentigines / seborrheic keratoses and lichen-planus like keratoses, bkl), dermatofibroma (df), melanoma (mel), melanocytic nevi (nv) and vascular lesions (angiomas, angiokeratomas, pyogenic granulomas and hemorrhage, vasc).\n",
        "\n",
        "More than 50% of lesions are confirmed through histopathology (histo), the ground truth for the rest of the cases is either follow-up examination (follow_up), expert consensus (consensus), or confirmation by in-vivo confocal microscopy (confocal).\n",
        "\n",
        "For this exercise we have downsampled the images to 100x75x3 and randomly taken 1400 samples from the original 10015 to make training times more reasonable. We have also made the dataset more balanced in terms of class distributions.\n",
        "\n",
        "Make sure you have downloaded the `HAM1400_100_75.zip` file from Canvas to your computer.\n",
        "\n",
        "Some tips and tricks (colab):\n",
        "* Copy the dataset to google drive and copy it to the colab workspace instead of uploading it. This will save you time, if don't finish the exercise in one go.\n",
        "* If you feel adventurous you can download and try some of the other resolutions and datasets (available in Canvas). There are 100x75, 256x192, 600x450 resolutions of images and the full datasets with ~10000 images and the \"downsampled\" datasets with 1400 images. If you use any of the other be prepared for looooong training times.\n",
        "* You may also want to experiment with changing runtime in google colab (too eg GPU) and see how much faster training will be. **PLEASE NOTE THAT YOU MIGHT GET THROWN OUT OF USING GPU/TPU RUNTIMES. ONLY THE CPU RUNTIMES ARE FREE.**\n",
        "\n",
        "We will start you off with loading the data and such and some (**maybe not all!**) of the relevant imports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "H0CWO-NpWavS"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m/Users/albertahnlide/Documents/GitHub/BMEN35_exercises/Session5/BMEN35_Ex16_deep_neural_networks_assignment5.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/albertahnlide/Documents/GitHub/BMEN35_exercises/Session5/BMEN35_Ex16_deep_neural_networks_assignment5.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Here you will upload the files\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/albertahnlide/Documents/GitHub/BMEN35_exercises/Session5/BMEN35_Ex16_deep_neural_networks_assignment5.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolab\u001b[39;00m \u001b[39mimport\u001b[39;00m files\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/albertahnlide/Documents/GitHub/BMEN35_exercises/Session5/BMEN35_Ex16_deep_neural_networks_assignment5.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m file \u001b[39m=\u001b[39m files\u001b[39m.\u001b[39mupload() \u001b[39m# Choose HAM1400_100_75.zip\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "# Here you will upload the files\n",
        "#from google.colab import files\n",
        "# file = files.upload() # Choose HAM1400_100_75.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "TjHOoORomDfb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cp: directory /content does not exist\n"
          ]
        }
      ],
      "source": [
        "# OPTIONAL IF YOU ALREADY HAVE COPIED THE DATASET TO GOOGLE DRIVE\n",
        "# This mounts your google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!cp -r /content/drive/MyDrive/data/HAM1400_100_75.zip /content/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "T0SMhKTtYBPQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "checkdir:  cannot create extraction directory: /content\n",
            "           Read-only file system\n"
          ]
        }
      ],
      "source": [
        "# This unzips the files\n",
        "!unzip -q /content/HAM1400_100_75.zip -d /content/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgT7_nihWgJq"
      },
      "source": [
        "Now we have the data in our workspace. Lets do some imports.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fwv_0oE-YbnG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from glob import glob\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.optimizers import SGD, RMSprop, Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras import regularizers\n",
        "from keras.applications.inception_v3 import InceptionV3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kq7LuDBm-Met"
      },
      "source": [
        "Now we will read the data into Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p91MOT7NYdbG"
      },
      "outputs": [],
      "source": [
        "# First read metadata into a dataframe\n",
        "base_dir = '/content/HAM1400_100_75/'\n",
        "skin_df=pd.read_csv(base_dir + 'dataframe.csv')\n",
        "# How many rows, that is how many images are there\n",
        "length = skin_df.shape[0]\n",
        "# Create a dictionary for the labels\n",
        "label_dict = {'nv': 0,'mel': 1,'bkl': 2, 'bcc': 3, 'akiec': 4,'vasc': 5,'df': 6 }\n",
        "# Recode labels in dataframe to be numbers\n",
        "skin_df['labels'] = skin_df['dx'].map(label_dict)\n",
        "#Allocate space for X and y aka our data and labels\n",
        "X = np.zeros((length,100,75,3))\n",
        "y = np.zeros((length))\n",
        "k = 0\n",
        "for i in skin_df['image_id']: # Get filename from dataframe\n",
        "  #print(k)\n",
        "  X[k,:,:,:] = np.asarray(Image.open(base_dir +  i + '.jpg'))\n",
        "  k= k+1\n",
        "\n",
        "y = np.asarray(skin_df['labels'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5Zbv5kq-VOH"
      },
      "source": [
        "We will do the usual conversion as before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cc9qz1j3ahe9"
      },
      "outputs": [],
      "source": [
        "# Convert data to float and scale to be between 0 and 1\n",
        "X = X.astype('float32')\n",
        "X /=255.0\n",
        "yidx = y\n",
        "y = to_categorical(y) # One-hot encode the labels\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gmy5WzlSY542"
      },
      "source": [
        "Let's plot the data and visualise what kind of data we are dealing with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJ93BQlqZyMO"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16, 14))\n",
        "plt.subplot(3, 3, 1)\n",
        "plt.title('Example of melanocytic nevi (nv)')\n",
        "plt.imshow(X[np.where(yidx == 0)[0][0], :, :])\n",
        "plt.subplot(3, 3, 2)\n",
        "plt.title('Example of melanoma (mel)')\n",
        "plt.imshow(X[np.where(yidx == 1)[0][0], :, :])\n",
        "plt.subplot(3, 3, 3)\n",
        "plt.title('Example of benign keratosis-like lesions (bkl)')\n",
        "plt.imshow(X[np.where(yidx == 2)[0][0], :, :])\n",
        "plt.subplot(3, 3, 4)\n",
        "plt.title('Example of basal cell carcinoma (bcc)')\n",
        "plt.imshow(X[np.where(yidx == 3)[0][0], :, :])\n",
        "plt.subplot(3, 3, 5)\n",
        "plt.title('Actinic keratoses and intraepithelial carcinoma (akiec)')\n",
        "plt.imshow(X[np.where(yidx == 4)[0][0], :, :])\n",
        "plt.subplot(3, 3, 6)\n",
        "plt.title('Example of vascular lesions (vasc)')\n",
        "plt.imshow(X[np.where(yidx == 5)[0][0], :, :])\n",
        "plt.subplot(3, 3, 7)\n",
        "plt.title('Example of dermatofibroma (df)')\n",
        "plt.imshow(X[np.where(yidx == 6)[0][0], :, :])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTDCM98Ie5MC"
      },
      "source": [
        "Now, you have data X_train, X_test and labels y_train, y_test. Create a deep learning model or (even better) use transfer learning on a pretrained model eg. Inception or any other found in eg. https://keras.io/api/applications/ ), train it properly and make predictions using it. You need get extract important metrics as well.\n",
        "\n",
        "To be clear you need to define the model and the metrics. For example the 'classification_report' in sklearn is good way to check performance. Plotting a confusion matrix is another.\n",
        "\n",
        "Make sure that your trained model doesn't get stuck in thinking everything is the same class.\n",
        "\n",
        "Remember that this dataset is in color. That means your images has a \"depth\". Make sure you define this appropriately when you define your model. Keep in mind the number of classes/targets you have here and in what format (\"one-hot encoded eg [1 0 0, 0 1 0, 0 0 1] or as \"numbers\" [0,1,2].\n",
        "\n",
        "**When you have finished this assignment , save this notebook and submit it in Canvas. For the submission you only need to use the HAM1400_100_75 dataset and show the results (confusion matrix or classification report or similar, accuracy alone is not ok).**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
