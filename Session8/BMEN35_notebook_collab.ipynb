{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ErikHartman/BMEN35/blob/2023/BMEN35_notebook_collab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5__sM-AKzFI6"
      },
      "source": [
        "# Exercise about xAI and omics\n",
        "\n",
        "This exercise is about methods in the field of explainable AI (xAI) and they're application to omics data. Specifically, we'll use SHapley Additive exPlanations (SHAP) to introspect trained models to understand their reasoning during classification tasks.\n",
        "\n",
        "The exercise has two parts:\n",
        "\n",
        "1. In the first part, you'll get to train a neural network on the MNIST dataset (digits 0-9). Thereafter you'll explain a set of predictions using SHAP. \n",
        "2. In the second part, we'll apply a similar methodology but to a clinical proteomic dataset. Here, SHAP is used to explain what proteins a classifier deems important when classifying two different subtypes of septic acute kidney injury.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "we2durP96qaf",
        "outputId": "c97cfecd-c237-4925-d81e-9d86470524a5"
      },
      "outputs": [],
      "source": [
        "!pip install shap\n",
        "!pip install xgboost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here you will train a convolutional neural network to classify the digits 0-9 (MINST dataset). Thereafter you will interpret the network using SHAP. Familiarize yourself with the code and make sure you understand every step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBngwm_3_uOS",
        "outputId": "cb9b237a-69a3-4faa-d1cc-1e44db789f13"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "\n",
        "num_classes = 10  # 0-9\n",
        "input_shape = (28, 28, 1)  # pixels\n",
        "\n",
        "# Load the data and split it between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Scale images to the [0, 1] range\n",
        "x_train = x_train.astype(\"float32\") / 255\n",
        "x_test = x_test.astype(\"float32\") / 255\n",
        "# Make sure images have shape (28, 28, 1)\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(x_train.shape[0], \"train samples\")\n",
        "print(x_test.shape[0], \"test samples\")\n",
        "\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 2\n",
        "\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        layers.Input(shape=input_shape),\n",
        "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Test accuracy:\", score[1])\n",
        "print(\"---- Model is trained, time for interpretation ----\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now it's time to interpret using SHAP. \n",
        "\n",
        "The background is a representative set of datapoints which should capture the \"average output\" of the network. This is simply to establish a baseline that outputs can be compared against later when explaining the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        },
        "id": "OS6b_ogEAI0y",
        "outputId": "063e39c1-b569-4818-b974-6aef637c2c93"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "\n",
        "# select a set of background examples to take an expectation over\n",
        "background = x_train[np.random.choice(x_train.shape[0], 100, replace=False)]\n",
        "\n",
        "# Create the explainer object\n",
        "e = shap.DeepExplainer(model, background)\n",
        "\n",
        "# Compute the SHAP values\n",
        "shap_values = e.shap_values(x_test[1:5])\n",
        "# Show them in an image\n",
        "shap.image_plot(shap_values, -x_test[1:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question: what features seem important for the different digits? You can change the x_test slice to see other digits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2\n",
        "\n",
        "Now it's time to apply the same principles but to some biological data.\n",
        "\n",
        "The data used here is the septic AKI data (see lecture)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "u7NPFb5b1a73"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# load the data from GitHub\n",
        "data_matrix = pd.read_csv(\n",
        "    \"https://raw.githubusercontent.com/ErikHartman/BMEN35/2023/data/quant_matrix.csv\"\n",
        ")\n",
        "design_matrix = pd.read_csv(\n",
        "    \"https://raw.githubusercontent.com/ErikHartman/BMEN35/2023/data/inner_design_matrix.tsv\",\n",
        "    sep=\"\\t\",\n",
        ")\n",
        "human_proteome = pd.read_csv(\n",
        "    \"https://raw.githubusercontent.com/ErikHartman/BMEN35/2023/data/human_proteome.gz\",\n",
        "    sep=\",\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9Z4zII2xF5bX"
      },
      "outputs": [],
      "source": [
        "def get_proteins_triv_name(proteins, proteome):\n",
        "    # function to map Uniprot IDs to more common names.\n",
        "    proteome[\"accession\"] = proteome[\"accession\"].apply(lambda x: x.split(\"_\")[0])\n",
        "    names = []\n",
        "    for protein in proteins:\n",
        "        if protein in proteome[\"accession\"].values:\n",
        "            m = proteome.loc[proteome[\"accession\"] == protein][\"trivname\"].values\n",
        "            assert len(m) == 1\n",
        "            m = m[0].split(\"_\")[0]\n",
        "        else:\n",
        "            m = protein\n",
        "        names.append(m)\n",
        "    return names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "C4k9rQ7v6ud2",
        "outputId": "01e97836-3f75-4854-b363-47e8389770b4"
      },
      "outputs": [],
      "source": [
        "group_one_samples = design_matrix[design_matrix[\"group\"] == 1][\"sample\"].values\n",
        "group_two_samples = design_matrix[design_matrix[\"group\"] == 2][\"sample\"].values\n",
        "data_matrix.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preprocess the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "vsP_id_DAaUk",
        "outputId": "5174aa88-8608-498a-a536-3eef545b19f9"
      },
      "outputs": [],
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "protein_labels = get_proteins_triv_name(\n",
        "    data_matrix[\"Protein\"].values, human_proteome\n",
        ")  # the UniProt IDs are translated to more common names.\n",
        "\n",
        "df1 = data_matrix[group_one_samples].T\n",
        "df2 = data_matrix[group_two_samples].T\n",
        "df1.columns = protein_labels\n",
        "df2.columns = protein_labels\n",
        "\n",
        "y = np.array(\n",
        "    [0 for _ in group_one_samples] + [1 for _ in group_two_samples]\n",
        ")  \n",
        "df_X = pd.concat([df1, df2]).fillna(0)\n",
        "X = df_X.to_numpy()\n",
        "scaler = preprocessing.StandardScaler().fit(X)  # makes mean = 0 and variance = 1\n",
        "X_scaled = scaler.transform(X)  # this is our scaled X\n",
        "\n",
        "df_X_scaled = pd.DataFrame(X_scaled)  # puts everything into a pandas DataFrame\n",
        "df_X_scaled.columns = df_X.columns\n",
        "df_X_scaled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "MH-tBvF58BFR",
        "outputId": "b076508a-347c-49f3-d597-6210791a8123"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\" Train and test how accuarge the XGBoost model is on the data here.\\nImport the necessary packages. Sci-kit learn is available by default in Google Collab\\nso there is no need to !pip install if you're using that package. \""
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from xgboost.sklearn import XGBClassifier\n",
        "\n",
        "\"\"\" Train and test how accuarge the XGBoost model is on the data here.\n",
        "Import the necessary packages. Scikit-learn is available by default in Google Collab\n",
        "so there is no need to !pip install if you're using that package. \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 974
        },
        "id": "zppyD2jIDXU5",
        "outputId": "eeb50d0d-df69-489e-ccf2-4c1bf8fc361f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\" Create some SHAP plots here! Feel free to stylize them and do whatever you'd like\""
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\" Create some SHAP plots here! Feel free to stylize them and do whatever you'd like. \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The last task is to look at your top scoring proteins. Can you find any reference or other information linking these to septic AKI? Write in markdown below."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
