{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antfolk/BMEN35_2023/blob/main/Session6/BMEN35_ensamble_cinc_assignment6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkiFi80bGKEc"
      },
      "source": [
        "# Assignment 6\n",
        "## Fill in your name below\n",
        "Albert Ahnlide\n",
        "\n",
        "## Your mission is now the following:\n",
        "\n",
        "You will use data from the Computing in Cardiology challenge 2022 (as was explained in the lectures) (https://moody-challenge.physionet.org/2022/). The training set contains data from 942 patients.\n",
        "\n",
        "We have done some preprocessing of the data so you have features and labels for both **murmur** and **outcome**. The features include age, sex, weight, heigth, pregnancy status and mean, variance and skewness for the phonocardiogram at five different locations.\n",
        "\n",
        "Evaluate different ensamble methods (at least 3) from sklearn (https://scikit-learn.org/stable/modules/ensemble.html and https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble) and see how they perform on the CinC2022 challenge data. Take some time to read the documentation and see what options are available.\n",
        "\n",
        "As you may remember from the lecture there is quite an eloborate scoring scheme, however this is handled by the file cinc2022metric.py and the methods contained therein.\n",
        "\n",
        "**Also remeber you have two sets of labels!!! One set of labels for murmurs (Present, Unknown, Absent) and one for outcomes (Abnormal, Normal).**\n",
        "\n",
        "**Another thing you need to take into account is that the scoring functions need label probabilities of the predicted classes.**\n",
        "\n",
        "You will also need one-hot encoding of \"hard\" values for the training labels and for the test labels.\n",
        "\n",
        "We will start by uploading some files. You need to upload the files cinc2022metrics.py, feats.csv, murmur_labels.csv and outcome_labels.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRiQuM1OHKQR"
      },
      "outputs": [],
      "source": [
        "#from google.colab import files\n",
        "#_ = files.upload() # Upload the other files available in github under Session 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YbyBrB9MKBW"
      },
      "source": [
        "Next we will import some of the libraries/modules needed. (You will need to import others later on)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SJx0_S_GMLfV"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/albertahnlide/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "import cinc2022metrics as cm # Our own little metrics file\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgS-HYV4MWn3"
      },
      "source": [
        "Next we will import data and the two different sets of labels and switch to numpy arrays.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NXy1fcB8Mah0"
      },
      "outputs": [],
      "source": [
        "feats = pd.read_csv('feats.csv', header=None)\n",
        "murmur_labels = pd.read_csv('murmur_labels.csv', header=None)\n",
        "outcome_labels = pd.read_csv('outcome_labels.csv', header=None)\n",
        "\n",
        "feats = feats.to_numpy()\n",
        "murmur_labels = murmur_labels.to_numpy()\n",
        "outcome_labels = outcome_labels.to_numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6AjJag6bbYG"
      },
      "source": [
        "Here we will split the data and also define what the different classes for murmur and outcome are."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "DYFYliDGNCl0"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train_murmur, y_test_murmur, y_train_outcome, y_test_outcome = train_test_split(feats, murmur_labels, outcome_labels, test_size=0.2, random_state=0)\n",
        "murmur_classes = ['Present', 'Unknown', 'Absent']\n",
        "outcome_classes = ['Abnormal', 'Normal']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AaRI3joNGvd"
      },
      "source": [
        "Next you can try out some of the available ensemble methods. Remember you need to predict probabilities for both classifiers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "FT0zGggANGIW"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "__init__() got an unexpected keyword argument 'n_estimators'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/Users/albertahnlide/Documents/GitHub/BMEN35_exercises/Session6/BMEN35_ensamble_cinc_assignment6.ipynb Cell 11\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/albertahnlide/Documents/GitHub/BMEN35_exercises/Session6/BMEN35_ensamble_cinc_assignment6.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mensemble\u001b[39;00m \u001b[39mimport\u001b[39;00m VotingClassifier \u001b[39mas\u001b[39;00m es \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/albertahnlide/Documents/GitHub/BMEN35_exercises/Session6/BMEN35_ensamble_cinc_assignment6.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Define and train RandomForestClassifier for murmur\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/albertahnlide/Documents/GitHub/BMEN35_exercises/Session6/BMEN35_ensamble_cinc_assignment6.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m es_murmur \u001b[39m=\u001b[39m es(n_estimators\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/albertahnlide/Documents/GitHub/BMEN35_exercises/Session6/BMEN35_ensamble_cinc_assignment6.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m es_murmur\u001b[39m.\u001b[39mfit(X_train, y_train_murmur)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/albertahnlide/Documents/GitHub/BMEN35_exercises/Session6/BMEN35_ensamble_cinc_assignment6.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Define and train RandomForestClassifier for outcome\u001b[39;00m\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'n_estimators'"
          ]
        }
      ],
      "source": [
        "# Do some imports (you now how to do this by now)\n",
        "from sklearn.ensemble import VotingClassifier as es \n",
        "\n",
        "# Define and train RandomForestClassifier for murmur\n",
        "es_murmur = es(n_estimators=100)\n",
        "es_murmur.fit(X_train, y_train_murmur)\n",
        "\n",
        "# Define and train RandomForestClassifier for outcome\n",
        "es_outcome = es(n_estimators=100)\n",
        "es_outcome.fit(X_train, y_train_outcome)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_murmur_prob = es_murmur.predict_proba(X_test)\n",
        "y_pred_outcome_prob = es_outcome.predict_proba(X_test)\n",
        "\n",
        "\n",
        "y_test_murmur_bin = to_categorical(y_test_murmur.argmax(axis=1), num_classes=len(murmur_classes)) \n",
        "y_test_outcome_bin = to_categorical(y_test_outcome.argmax(axis=1), num_classes=len(outcome_classes))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MzLjkQXb0-d"
      },
      "source": [
        "Now you should also One-hot encode the **test** (not the predicted ones) labels for both murmur and outcome."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "E0LHwnQUb0fV"
      },
      "outputs": [],
      "source": [
        "murmur_scores, outcome_scores = cm.compute_scores(y_test_murmur_bin,   # One-hot encoded test labels for murmur eg. [1 0 0]\n",
        "                                                  y_pred_murmur_prob,  # One-hot encoded predicted probabilities for murmur eg. [0.1 0.7 0.2]\n",
        "                                                  murmur_classes,      # As defined before\n",
        "                                                  y_test_outcome_bin,  # One-hot encoded test labels for outcome eg. [1 0]\n",
        "                                                  y_pred_outcome_prob, # One-hot encoded predicted probabilities for outcome eg. [0.1 0.9]\n",
        "                                                  outcome_classes)     # As defined before\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T91QCcRgcl9T"
      },
      "source": [
        "Now we have calculated a whole bunch of scores for both murmur and outcome. We can print them using the following."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Vq_XJnNot7qH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#Murmur scores\n",
            "AUROC,AUPRC,F-meas,Accuracy,Weighted Accuracy,Cost\n",
            "nan,1.000,0.017,0.026,0.026,19992.416\n",
            "\n",
            "#Outcome scores\n",
            "AUROC,AUPRC,F-measure,Accuracy,Weighted Accuracy,Cost\n",
            "nan,1.000,0.330,0.492,0.492,30794.067\n",
            "\n",
            "#Murmur scores (per class)\n",
            "Classes,Present,Unknown,Absent\n",
            "AUROC,nan,nan,nan\n",
            "AUPRC,1.000,nan,nan\n",
            "F-measure,0.052,0.000,0.000\n",
            "Accuracy,0.026,nan,nan\n",
            "\n",
            "#Outcome scores (per class)\n",
            "Classes,Abnormal,Normal\n",
            "AUROC,nan,nan\n",
            "AUPRC,1.000,nan\n",
            "F-measure,0.660,0.000\n",
            "Accuracy,0.492,nan\n",
            "\n"
          ]
        }
      ],
      "source": [
        "cm.print_scores(murmur_scores, outcome_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGczxkWWR50z"
      },
      "source": [
        "Did you manage to comparable scores to those in the leaderboard as shown in the last slide of the lecture? (It should be noted that we don't have the validation data so we are perhaps comparing apples and oranges). Which set of ensemble classifiers worked the best?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5zNz7eZeSvC"
      },
      "source": [
        "## The End"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
